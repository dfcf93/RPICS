module pa3;

import java.io.*;
import java.util.*;
/* 
 *  Index.salsa --
 *  Given a directory, workers, and documents, calculate the tfidf for all documents
 *  Process
 *  1. Create M amount of workers and distribute documents amongst them as evenly as possible
 *  2. Run distribute1 which produces the tf for all the items within all the documents to be read
 *  3. Merge the actors results and then calculate the idf for all the the words
 *  4. Run distribute2 which produces the tfidf for all words in each document
 *  5. Merge the actor results into a hashtable of hashtables containing all the tfidfs
 *  6. Convert our hashmap of all the words into an alphabetically sorted vector
 *  7. Create 26 actors(one for each letter) and print the results to their respective documents(a.txt, b.txt, etc)
 */
behavior Index{
    String collectionDir = "collection/";
    String outputDir = "index/";
    int filesToBeIndexed = 10;
    int noActors = 2;
    int documentsPerActor = 0;
    long initialTime;

    void act(String args[]){
        initialTime=System.currentTimeMillis();
        standardOutput<-println(">>>>>>Starting the computation");
        int argc=args.length;
        if(args.length == 4) {
            collectionDir = args[0];
            outputDir = args[1];
            filesToBeIndexed = Integer.parseInt(args[2]);
            noActors = Integer.parseInt(args[3]);
            documentsPerActor = filesToBeIndexed/noActors;
            if (noActors > filesToBeIndexed) {
                noActors = filesToBeIndexed;
                documentsPerActor = 1;
            }
            makeOutputDir();
            actHelper();
        } else {
            exitFailure();
        }
    }

    void actHelper() {
        IndexWorker[] workers = new IndexWorker[noActors];
        for(int i=0;i<noActors;i++){
            workers[i]=new IndexWorker();
        }
        distribute1(workers)@distribute2(token, workers)@writeResults(token)@endTimer();
    }

    void writeResults(Hashtable terms) {
        WriterWorker[] workers = new WriterWorker[26];
        for(int i=0;i<26;i++){
            workers[i]=new WriterWorker();
        }
        Vector words = ((Vector) terms.get("allwords"));
        terms.remove("allwords");
        for(int i=0;i<26;i++) {
            String letter = String.valueOf((char)(i + 97));
            workers[i]<-writeTFIDF(letter, outputDir, terms, words);
        }
    }

    void endTimer(){
        long finalTime=System.currentTimeMillis();
        long runningTime=finalTime-initialTime;
        standardOutput<-println("Running time to build the Index: "+runningTime+" ms.");
    }

    void makeOutputDir() {
        File theDir = new File(outputDir);
        if (!theDir.exists()) {
            try{
                theDir.mkdir();
             } catch(SecurityException se){
                standardOutput<-println("[error] Unable to create output folder");
             }        
        }
    }

    Vector hashToSortedVector(Hashtable words) {
        Vector vec = new Vector(words.keySet());
        Collections.sort(vec);
        return vec;
    }

    double idf(int documents, int occurrences) {
        return Math.log10((((double) documents) + 2)/(1 +  ((double) occurrences)));
    }

    Hashtable distribute1(IndexWorker[] workers){
        int startnumber = 0;
        int endnumber = startnumber + documentsPerActor;
        join {
            for(int i=0;i<noActors-1;i++){
                workers[i]<-readFromAssignedFiles(startnumber, endnumber, collectionDir);
                startnumber=endnumber;
                endnumber=startnumber + documentsPerActor;
            }
            workers[noActors-1]<-readFromAssignedFiles(startnumber, filesToBeIndexed-1,collectionDir);
        }@combine1(token)@currentContinuation;
    }

    Hashtable distribute2(Hashtable terms, IndexWorker[] workers) {
        Hashtable words = ((Hashtable) terms.get("allwords"));
        Hashtable workerTerms = new Hashtable();
        terms.remove("allwords");
        Object[] array = terms.keySet().toArray();
        int startnumber = 0;
        int endnumber = startnumber + documentsPerActor;
        String s;
        join {
            for(int i=0;i<noActors-1;i++){
                workerTerms.clear();
                for (int j=startnumber;j<endnumber;j++) {
                    s = String.valueOf(j)+".txt";
                    workerTerms.put(s,terms.get(s));
                }
                workers[i]<-calculateTFIDF(workerTerms, words);
                startnumber=endnumber;
                endnumber=startnumber + documentsPerActor;
            }
            workerTerms.clear();
            for (int i=startnumber;i<filesToBeIndexed;i++) {
                s = String.valueOf(i)+".txt";
                workerTerms.put(s,terms.get(s));
            }
            workers[noActors-1]<-calculateTFIDF(workerTerms, words);
        }@combine2(token,words)@currentContinuation;
    }

    Hashtable calculateIDF(Object t2) {
        Hashtable terms = ((Hashtable) t2);
        Hashtable words = ((Hashtable) terms.get("allwords"));
        terms.remove("allwords");
        Object[] array = words.keySet().toArray();
        int amountOfDocs = terms.size();
        for (int j = 0; j < array.length; j++) {
            int occurences = 0;
            for(int i = 0; i < amountOfDocs; i++) {
                String s = String.valueOf(i) + ".txt";
                Hashtable t = ((Hashtable) terms.get(s));
                if (t.containsKey(array[j])) {
                    occurences++;
                }
            }
            words.put(array[j],idf(amountOfDocs, occurences));   
        }
        terms.put("allwords",words);
        return terms;
    }

    Hashtable combine2(Object[] results, Hashtable words) {
        Hashtable terms = new Hashtable();
        Vector words1 = hashToSortedVector(words);
        for (int i = 0; i<results.length; i++) {
            Hashtable t = ((Hashtable) results[i]);
            Object[] array = t.keySet().toArray();
            for (int j = 0; j < array.length; j++) {
                terms.put(array[j], t.get(array[j]));
            }
        }
        terms.put("allwords",words1);
        return terms;
    }

    Hashtable combine1(Object[] results) {
        Hashtable all = new Hashtable();
        Hashtable terms = new Hashtable();
        for (int i = 0; i<results.length; i++) {
            Hashtable t = ((Hashtable) results[i]);
            Object[] array = t.keySet().toArray();
            Hashtable t2 = merge(all, ((Hashtable) t.get("allwords")));
            all.clear();
            all = t2;
            for (int j = 0; j < array.length; j++) {
                if (!array[j].equals("allwords")) {
                    terms.put(array[j], t.get(array[j]));
                } else {
                    terms.put(array[j], t.get(array[j]));
                }
            }
        }
        terms.put("allwords",all);
        return calculateIDF(terms);
    }

    Hashtable merge(Hashtable table1, Hashtable table2) {    
        Object[] array = table1.keySet().toArray();
        for (int j = 0; j < array.length; j++) {
            if (!table2.containsKey(array[j])) {
                table2.put(array[j],array[j]);
            }
        }
        return table2;
    }
}
